from urllib.request import urlopen, Request
from urllib.parse import urlparse
import sys
import os
import threading
import queue

# List of infected WordPress URL
filename = "list.txt"
TIMEOUT = 8
UA = 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 Firefox/64.0'
WP_paths = ['/wp-content', '/wp-includes', '/wp-admin', '', '/wp-includes/fonts', '/wp-includes/js', '/wp-content/plugins',
            '/wp-includes/customize', '/wp-admin/css', '/wp-admin/css/colors', '/wp-content/uploads', '/wp-admin/user',
           '/wp-admin/js', '/wp-admin/includes', '/wp-admin/network']
Webshells = [
    'user.php',
    'common.php',
    'import.php',
    'update.php',
    'link.php',
    'license.php',
    'menu.php',
    'image.php',
    'options.php',
    'tools.php',
    'core.php',
    'edit.php',
    'functions.php',
    'config.php']

def process_reply(reply, website):
    if "<form method=post>Password" in reply:
        print("[Webshell found] " + website)

def crawl(q):
	while q.qsize() >0:
		item = q.get()
		if(validate_connection(item[0])):
			for paths in item:
				request = Request(paths, headers={'User-Agent': UA})
				try:
					response = urlopen(request, timeout=TIMEOUT)
					process_reply(response.read().decode("utf8"), paths)
				except Exception as e:
					pass
					#print("[Status] ", e , paths)
	
def validate_connection(website):
    try:
        urlopen(Request(website, headers={'User-Agent': UA}), timeout=TIMEOUT)
        return True
    except Exception as e:
            print("[Problem connecting, abording] ", website)
            return False

def parse_url(url):
    url = url.replace("hxxp://", "http://")
    url = url.replace("hxxps://", "https://")
    o = urlparse(url)
    url_without_query_string = o.scheme + "://" + o.netloc
    url_without_query_string = url_without_query_string.rstrip("/")
    return url_without_query_string

def usage(code=0):
    print('Usage: ' + os.path.basename(__file__) + ' list.txt')
    print('The list should contain the infected WordPress URLs')
    exit(code)

if len(sys.argv) != 2:
    usage(1)

filename = sys.argv[1]
urls_list=[]
q = queue.Queue()

with open(filename) as f:
    urls_list=[]
    for line in f:
        url = parse_url(line.strip())
        urls_list.append(url)

for i in range(len(urls_list)):
	urls_list_for_that_site=[]
	urls_list_for_that_site.append(urls_list[i])
	for paths in WP_paths:
		for webshell in Webshells:
			current_website=urls_list[i]			
			urls_list_for_that_site.append(current_website + paths + "/" + webshell)
	q.put(urls_list_for_that_site)	

print(str(len(urls_list)) + " websites will be tested")

for i in range(10):
	threading.Thread(target=crawl, args=(q,)).start()
	
