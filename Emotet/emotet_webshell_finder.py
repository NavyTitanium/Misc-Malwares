from urllib.request import urlopen, Request
from urllib.parse import urlparse
import sys
import os
import threading
import queue

# List of infected WordPress URL
filename = "list.txt"
TIMEOUT = 8
reason_ok = ['404', '403', '401', '500']
UA = 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 Firefox/64.0'
WP_paths = [
    'wp-content',
    'wp-includes',
    'wp-admin',
    '',
    'wp-includes/fonts',
    'wp-includes/js',
    'wp-content/plugins',
    'wp-includes/customize',
    'wp-includes/images',
    'wp-includes/widgets',
    'wp-admin/css',
    'wp-admin/css/colors',
    'wp-content/uploads',
    'wp-admin/user',
    'wp-admin/js',
    'wp-admin/includes',
    'wp-admin/network']
Webshells = [
    'user.php',
    'common.php',
    'import.php',
    'update.php',
    'link.php',
    'license.php',
    'menu.php',
    'image.php',
    'options.php',
    'tools.php',
    'core.php',
    'edit.php',
    'functions.php',
    'config.php',
    'wp-list.php',
    '5e3.php']


def process_reply(reply, website):
    if "<form method=post>Password" in reply:
        print("[Webshell found] " + website)


def crawl(q):
    while q.qsize() > 0:
        item = q.get()
        if(validate_connection(item[0])):
            for paths in item:
                request = Request(paths, headers={'User-Agent': UA})
                try:
                    response = urlopen(request, timeout=TIMEOUT)
                    process_reply(response.read().decode("utf8"), paths)
                except Exception as e:
                    pass
                    # print("[Status] ", e , paths)


def validate_connection(website):
    try:
        urlopen(Request(website, headers={'User-Agent': UA}), timeout=TIMEOUT)
        return True
    except Exception as e:
        error = str(e)
        for code in reason_ok:
            if code in error:
                return True
#        print(e)
#        print("[Problem connecting, abording] ", website)
        return False


def parse_url(url):
    a = []
    url = url.replace("hxxp://", "http://")
    url = url.replace("hxxps://", "https://")
    url = url.replace("[.]", ".")
    o = urlparse(url)
    url_without_query_string = o.scheme + "://" + o.netloc
    url_without_query_string = url_without_query_string.rstrip("/")
    a.append(url_without_query_string)
    full_url = (o.scheme + "://" + o.netloc + o.path).rstrip("/")
    full_url = full_url.split("/")
    full_url.pop(0)
    full_url.pop(0)
    full_url.pop(0)

    if(len(full_url) > 1 and not bool(set(full_url).intersection(WP_paths))):
        full_url.pop()
        u = url_without_query_string
        for p in full_url:
            u = u + "/" + p
            a.append(u)

    return a


def usage(code=0):
    print('Usage: ' + os.path.basename(__file__) + ' list.txt')
    print('The list should contain the infected WordPress URLs')
    exit(code)


if len(sys.argv) != 2:
    usage(1)

filename = sys.argv[1]
urls_list = []
q = queue.Queue()

with open(filename) as f:
    urls_list = []
    for line in f:
        t = []
        t = parse_url(line.strip())
        urls_list.append(t)

paths_generated=[]

for i in range(len(urls_list)):
    urls_list_for_that_site = []
    urls_list_for_that_site.append(urls_list[i][0])
    for paths in WP_paths:
        for webshell in Webshells:
            current=urls_list[i][0] + "/" + paths + "/" + webshell
            if(current not in paths_generated):
	            urls_list_for_that_site.append(current)
        	    paths_generated.append(current)

    urls_list[i].pop(0)
    for url in urls_list[i]:
        for webshell in Webshells:
            current=url + "/" + webshell
            if(current not in paths_generated):
                    urls_list_for_that_site.append(current)
                    paths_generated.append(current)

    q.put(urls_list_for_that_site)

print(str(len(urls_list)) + " websites will be tested with " + str(len(paths_generated)) +  " URLS generated" )
paths_generated=[]

for i in range(35):
    threading.Thread(target=crawl, args=(q,)).start()
